# Apache Spark
A memory-first, distributed runtime engine for executing highly-scalable data processing. Spark can run on YARN, Mesos, or as a standalone Spark cluster; normally people choose to run on existing YARN clusters.

## Pre-requisites
- Functional Programming Fundamentals
- Scala or Python
- SQL and Databases
- Data Warehousing Principles

## Top 3 Things to Learn for Spark:
- RDD: the most basic representation of 
- DataFrames: Spark SQL and the Unified API for Batch and Streaming
- Connectors: libraries for accessing different data sources

## Runtimes for Spark
- HDP: Hadoop ecosystem installed using Ambari
- AWS EMR: Managed YARN cluster
- GCP DataProc: Managed YARN cluster
